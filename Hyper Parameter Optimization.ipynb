{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter optimization is the task of finding an optimal or near optimal (locally) set of hyper-parameters (or free parameters that are set manually or externally outside of a learning algorithm's self-adjustment of its internal parameters). In other words, we are searching for a good configuration of the various \"knobs\" one must set to achieve good generalization/out-of-sample performance.\n",
    "\n",
    "To learn more about Convolutional Neural networks refer to this <a href= 'https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/'>Intutive Explanation.</a> \n",
    "Here, we create a Convolutional Neural Network trained on MNIST Dataset using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Only look at 2s and 7s\n",
    "train_picks = np.logical_or(y_train==2,y_train==7)\n",
    "test_picks = np.logical_or(y_test==2,y_test==7)\n",
    "\n",
    "x_train = x_train[train_picks]\n",
    "x_test = x_test[test_picks]\n",
    "y_train = np.array(y_train[train_picks]==7,dtype=int)\n",
    "y_test = np.array(y_test[test_picks]==7,dtype=int)\n",
    "\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Convolutional Neural Network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "model.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the values we get without Hyperparameter optimization:\n",
    "\n",
    "Test Loss# = 4.89%, \n",
    "Test Accuracy# = 98.34%\n",
    "('#' -> Results might differ)\n",
    "\n",
    "\n",
    "Here, there are various parameters that can be tweaked to improve the accuracy of the network. They are:\n",
    "1. Epochs.\n",
    "2. Batch Size.\n",
    "3. Optimization Algorithm.\n",
    "4. Network Weight Initialization.\n",
    "5. Activation Functions.\n",
    "6. Dropout Regularization.\n",
    "7. Number of Neurons.\n",
    "\n",
    "Keras models can be used in scikit-learn by wrapping them with the KerasClassifier or KerasRegressor class. In this example, we can use scikit-learn's GridSearchCV function to perform Hyper Parameter Optimization with KerasClassifier.\n",
    "\n",
    "\n",
    "It can be used as follows:\n",
    "```python\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def create_model():\n",
    "\t...\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine-learning parlance, an epoch is a complete pass through a given dataset. That is, by the end of one epoch, your neural network – be it a restricted Boltzmann machine, convolutional net or deep-belief network – will have been exposed to every record to example within the dataset once. Not to be confused with an iteration, which is simply one update of the neural net model’s parameters. Many iterations can occur before an epoch is over. Epoch and iteration are only synonymous if you update your parameters once for each pass through the whole dataset.\n",
    "\n",
    "In the neural network we used a value of 12 for the **Epoch** parameter. We will experiment with the values of epochs to see if we can improve the accuracy. We will select *[10,20,30,40]* as the values to be fed to the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Model that will be used for GridSearch\n",
    "def build_model(epochs):\n",
    "    print(\"\\nThe current number of epochs are {}\\n\".format(epochs))\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(4, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Assigning 'model' to KerasClassifier to be used with GridSearchCV\n",
    "model = KerasClassifier(build_fn = build_model)\n",
    "parameters = {'epochs': [10, 20, 30, 40]}\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 2)\n",
    "grid_search = grid_search.fit(x_train, y_train)\n",
    "print(\"\\nThe best parameter is {}\".format(grid_search.best_params_))\n",
    "print(\"\\nThe best_accuracy is {}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The best parameter is {'epochs':40}\n",
    "#The best_accuracy is 0.9937004008835801"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ***Loss VS Epochs*** and ***Accuracy Vs Epochs*** Graphs to understand what is happening\n",
    "<table><tr><td><img src=\"./Epochs/AccuracyEpoch.png\" width = \"1000\"/></td><td><img src=\"./Epochs/LossEpoch.png\" width =\"1000\"/></td></tr></table>\n",
    "\n",
    "From the above graphs we can conclude two things:\n",
    "1. The accuracy is increasing as number of Epochs increase.\n",
    "2. The loss is decreasing as number of Epochs increase.\n",
    "\n",
    "The Epochs in the neural network can be increased, but after certain number of epochs the accuracy won't increase. So while training a neural network if a **Flat line** is observed in either Accuracy or Loss, there is no meaning in increasing the number of Epochs as this won't increase the Accuracy by a great difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size defines number of samples that going to be propagated through the network.\n",
    "For instance, let's say you have 1000 training samples and you want to set up batch_size equal to 100. Algorithm takes first 100 samples (from 1st to 100th) from the training dataset and trains network. Next it takes second 100 samples (from 101st to 200th) and train network again. We can keep doing this procedure until we will propagate through the networks all samples. The selection of Batch Size is a tricky task because:\n",
    "\n",
    "1. The higher the batch size, the more memory space you'll need but the accuracy can be pretty good\n",
    "2. The smaller the batch the less accurate estimate of the gradient. In the figure below you can see that mini-batch (green color) gradient's direction fluctuates compare to the full batch (blue color).\n",
    "\n",
    "<img src = \"./BatchSize.png\" height = \"500\" width = \"700\" align= \"center\" />\n",
    "\n",
    "*Stochastic is just a mini-batch with batch_size equal to 1. Gradient changes its direction even more often than a mini-batch.*\n",
    "\n",
    "In the neural network, we used batch_size as 128. We will be experimenting with *[32, 64, 128, 192, 256]*  as batch_size values to be fed to the neural network to see it's effect on Loss and Accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Model that will be used for GridSearch\n",
    "def build_model(batch_size):\n",
    "    print(\"\\nThe current number of batch size is {}\\n\".format(batch_size))\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(4, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Assigning 'model' to KerasClassifier to be used with GridSearchCV\n",
    "model = KerasClassifier(build_fn = build_model)\n",
    "parameters = {'batch_size' : [32, 64, 128, 192, 256]}\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 2)\n",
    "grid_search = grid_search.fit(x_train, y_train)\n",
    "print(\"\\nThe best parameter is {}\".format(grid_search.best_params_))\n",
    "print(\"\\nThe best_accuracy is {}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The best parameter is {'batch_size': 128, 'epochs': 40}\n",
    "#The best_accuracy is 0.9947639695655731"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src=\"./Accuracy.png\" width = \"1000\"/></td><td><img src=\"./Loss.png\" width =\"1000\"/></td></tr></table>\n",
    "\n",
    "Instead of finding the best parameter independently, both Epochs and Batch Size can be given as input to the GridSearchCV function. There is one thing to be noted here: \n",
    "The Best parameter for Epoch is 40 and Batch Size is 128. However, the best parameters can truly be found after giving both Epoch and Batch size at the same time as input to the GridSearchCV function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_model(epochs,batch_size):\n",
    "    print(\"\\nThe current number of epochs are {}\\nThe current number of batch size is {}\\n\".format(epochs,batch_size))\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(4, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "model = KerasClassifier(build_fn = build_model)\n",
    "parameters = {'epochs': [10,20,30,40],\n",
    "              'batch_size' : [32,64,128,192,256]}\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 2)\n",
    "grid_search = grid_search.fit(x_train, y_train)\n",
    "print(\"\\nThe best parameter is {}\".format(grid_search.best_params_))\n",
    "print(\"\\nThe best_accuracy is {}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The best parameter is {'batch_size': 64, 'epochs': 40}\n",
    "#The best_accuracy is 0.9952548474188007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above we can see that we get an increase in accuracy, but this comes at a cost of heavy computation. GridSearchCV looks at all the parameters given to it via the parameter grid (*param_grid*) which leads to perform every combination in the parameters to find the best results. An alternative to this can be RandomizedSearchCV function. RandomizedSearchCV is computationally less expensive as it uses randomness to find the best attributes instead of trying each and every parameter in the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_model(epochs,batch_size):\n",
    "    print(\"\\nThe current number of epochs are {}\\nThe current number of batch size is {}\\n\".format(epochs,batch_size))\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(4, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "model = KerasClassifier(build_fn = build_model)\n",
    "parameters = {'epochs': [10,20,30,40],\n",
    "              'batch_size' : [32,64,128,192,256]}\n",
    "random_search = RandomizedSearchCV(estimator = model,\n",
    "                           param_distributions = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 2)\n",
    "random_search = random_search.fit(x_train, y_train)\n",
    "print(\"\\nThe best parameter is {}\".format(random_search.best_params_))\n",
    "print(\"\\nThe best_accuracy is {}\".format(random_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The best parameter is {'epochs': 20, 'batch_size': 128}\n",
    "#The best_accuracy is 0.9936185879080423"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, we are able to get an accuracy of **99.36%** which is just **0.1636%** less from the GridSearchCV but very faster.\n",
    "\n",
    "Conclusions:\n",
    "1. RandomizedSearchCV takes less time than GridSearchCV to find results which are pretty good and acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Optimization Algorithm\n",
    "\n",
    "Keras offers many optimization algorithms that can be used to train the Neural Network. A list of all optimizers with their Arguments are listed <a href = \"https://keras.io/optimizers/\"> here.</a>\n",
    "\n",
    "Here, we will be experimenting with SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam for improving the accuracy of the Neural Network. We will be using the best parameters from RandomizedSearchCV i.e batch_size = 128, epochs = 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_model(optimizer):\n",
    "    print(\"\\nThe current optimizer is {}\\n\".format(optimizer))\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(4, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "model = KerasClassifier(build_fn = build_model, epochs = 20 , batch_size = 128)\n",
    "parameters = {'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']}\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 2)\n",
    "grid_search = grid_search.fit(x_train, y_train)\n",
    "print(\"\\nThe best parameter is {}\".format(grid_search.best_params_))\n",
    "print(\"\\nThe best_accuracy is {}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The best parameter is {'optimizer': 'Nadam'}\n",
    "#The best_accuracy is 0.9936185879080423"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no hard and fast rule to choose an optimization algorithm. So, it is better to try everything. But, I'd suggest to look out for the latest optimizers implemented and try them first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Network Weight Initialization\n",
    "\n",
    "Usually weights in a Neural Network are selected to be random. But now there is a <a href = \"https://keras.io/initializers/#usage-of-initializers\">list</a> of Weight initializors in Keras that can be used. These do have an effect on the performance of the Neural Network.\n",
    "\n",
    "We'll be using uniform, lecun_uniform, normal, zero, glorot_normal, glorot_uniform, he_normal, he_uniform for finding the best accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_model(init_mode):\n",
    "    print(\"\\nThe current init_mode is {}\\n\".format(init_mode))\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(4, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='Nadam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "model = KerasClassifier(build_fn = build_model, epochs = 20 , batch_size = 128)\n",
    "parameters = {'init_mode': ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']}\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 2)\n",
    "grid_search = grid_search.fit(x_train, y_train)\n",
    "print(\"\\nThe best parameter is {}\".format(grid_search.best_params_))\n",
    "print(\"\\nThe best_accuracy is {}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The best parameter is {'init_mode': 'zero'}\n",
    "#The best_accuracy is 0.9949275955166489"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Activation Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
